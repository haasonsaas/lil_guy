---
author: Jonathan Haas
pubDate: '2025-01-07'
title: 'The AI Skill Mirror: Why Technical Interviews Need a Complete Rewrite'
description: "AI reveals the true skill level of its operator. Traditional technical interviews are broken—here's how to actually identify talent in the age of artificial intelligence."
featured: false
draft: false
tags:
  - ai
  - hiring
  - technical-interviews
  - skill-assessment
  - developer-productivity
---

Your technical interview process is broken. Not because the questions are too hard or too easy, but because the entire premise no longer applies to reality.

I've been thinking about this problem for months, watching candidates cruise through coding challenges with AI assistance while completely missing the point. The old gate-keeping methods don't work when ChatGPT can solve FizzBuzz in six different languages with perfect documentation.

But here's what I've discovered: AI doesn't make everyone equally skilled. It amplifies existing ability. It's a mirror that reflects the operator's true technical understanding back at them, magnified.

## The Fundamental Shift

Traditional interviews tested whether someone could solve problems. Now we need to test whether they can solve problems _with_ AI. The skill isn't in memorizing algorithms—it's in knowing which algorithm to ask for and recognizing when the AI got it wrong.

This isn't just about being "AI-friendly" in interviews. It's about recognizing that the job itself has fundamentally changed. A developer who can't effectively collaborate with AI in 2025 is like a developer who couldn't use Google in 2005—technically possible, but practically useless.

## What Actually Matters Now

The best developers I work with don't try to outcode the AI. They orchestrate it. They understand that the real skill is in:

**Context Management**: Can they feed the AI the right information at the right time? Do they understand how to structure prompts to get better output? When I watch someone iterate endlessly without improving their context, I know they're not going to scale.

**Quality Recognition**: This is the big one. Can they look at AI-generated code and immediately spot the problems? The subtle bugs, the performance issues, the maintainability concerns? Anyone can ask AI to "make it faster"—but can they recognize when it actually made it slower?

**System Thinking**: AI excels at local optimization but struggles with global architecture. The developers who succeed are the ones who can see the bigger picture, understand the trade-offs, and guide AI toward solutions that fit the entire system.

## The New Interview Framework

Here's what I ask candidates now:

**First, I want to see their fundamentals.** Not algorithm memorization, but deep understanding of how systems work. If they can't explain why this database query will be slow or why this API design will cause problems, they won't be able to guide AI effectively.

**Then I watch them dance with the AI.** Full screen share, real problem, AI tools allowed. I'm not watching to see if they can code—I'm watching to see if they can think. Do they ask the right questions? Do they iterate effectively? Do they catch the AI's mistakes?

**Most importantly, I look for curiosity.** Are they exploring edge cases? Are they questioning assumptions? Are they using AI to learn, not just to complete tasks? The best candidates teach me something new about how they use AI.

## The Red Flags

I've learned to spot the warning signs quickly:

- **Endless chatting with AI** without actually testing or iterating
- **Accepting AI output blindly** without understanding or validation
- **Over-reliance on AI completion** instead of thinking through problems
- **Inability to debug AI-generated code** when it inevitably breaks

These aren't just signs of poor AI usage—they're signs of poor engineering thinking that AI amplifies instead of fixing.

## The Cultural Question

Here's what still matters: Are they curious? Do they persist through difficult problems? Would you trust them with a customer? Do they have opinions about code quality?

AI doesn't change these fundamentals. If anything, it makes them more important. A curious developer will push AI to its limits and discover new capabilities. A lazy developer will use AI as an excuse to stop thinking.

## The Unsolved Problem

The biggest challenge isn't figuring out what to test—it's the economics. Watching someone work with AI for 2-3 hours gives you incredible signal, but it's expensive. You can't do this for every candidate.

The old funnel system is broken. Phone screens that used to filter out 90% of candidates are now useless when AI can answer any technical question. We need new filtering mechanisms that work at scale.

I don't have a perfect solution yet. But I know that companies still using traditional coding challenges are optimizing for the wrong skills. They're hiring for 2015, not 2025.

## What This Means For You

If you're a developer, start paying attention to how you work with AI. It's not just about getting code written faster—it's about developing the meta-skills that make you valuable in an AI-augmented world.

If you're hiring, stop testing whether candidates can solve problems without AI. Start testing whether they can solve problems better with AI than your current team can.

The mirror doesn't lie. The question is: what will it reflect when you look into it?
